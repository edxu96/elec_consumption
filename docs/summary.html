---

title: Summary


keywords: fastai
sidebar: home_sidebar



nb_path: "02_summary.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 02_summary.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>How to solve the tasks and why that way are discussed here.</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Physical unit is <a href="https://edxu96.github.io/elec_consumption/#1.-Introduction">presumed to be kiloWatt</a>.</p>
<p>The distribution of missing entries can be summarised as:</p>
<ul>
<li>For households 162, 428 and 432, two entries in different sets of dates are missed".</li>
<li>For all the other households, 48 entries in different sets of dates are missed.</li>
<li>There are only 186 time points when data is complete.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-Clustering">1. Clustering<a class="anchor-link" href="#1.-Clustering"> </a></h2><p>There are at least two challenges:</p>
<ul>
<li>Too many missing days distribute differently among units.</li>
<li>All series are non-stationary because of non-business-days, long-term trends, and seasonality.</li>
</ul>
<h3 id="Why-not-k-Means">Why not k-Means<a class="anchor-link" href="#Why-not-k-Means"> </a></h3><p>Missing entries must be filled with meaningfull values first, or algorithms, like k-Means do not work. Clearly, neither to fill with 0 nor to interpolate is a good choice in terms of time series.</p>
<p>Monte Carlo simulations can be used, but the resulted effect is not known. The argument is that most clustering algorithms rely on calculation of cluster centres, which involves multiple units at the same time. Nevertheless, the joint set of missing days from those units might be large compared to that from pair-wise units, so simulated entries exist in more time index, distorting cluster centres.</p>
<h3 id="Why-not-Agglomerative-Hierarchical-(Single-Linkage)">Why not Agglomerative Hierarchical (Single Linkage)<a class="anchor-link" href="#Why-not-Agglomerative-Hierarchical-(Single-Linkage)"> </a></h3><p>Agglomerative hierarchical clustering (AHC) using single linkage relies on some distance matrix resulted from original data only. Entries in such distance matrix measure the similarity of some unit pair according to a pre-defined criterion. For similarity between time series, Pearson correlation, instead of Euclidean distance, is usually used.</p>
<p>When number of clusters is set to 4, for example, differences in sizes -- 497, 1, 1, 1 respectively -- are significant. The reason is that units in last three clusters do not have strong correlation with any other unit. That is, they are outliers. Though their distribution among clusters does not have a huge impact, they prevent the algorithm to process units in the first big cluster.</p>
<h3 id="My-Approach:-Max-Spanning-Tree-on-Distance-Matrix">My Approach: Max Spanning Tree on Distance Matrix<a class="anchor-link" href="#My-Approach:-Max-Spanning-Tree-on-Distance-Matrix"> </a></h3><p>AHC using single linkage is an optimisation problem in essence. To run max spanning tree on distance matrix yields better result, actually. Another advantage of this approach is that the complete graph (with lightgrey-coloured edges in the following figure), which represesnts the distance matrix, provides a more intuitive way to choose clusters.</p>
<p>Units 70 ~ 99 are considered in the following example, resulting a clearer figure:</p>
<ul>
<li>The max spanning tree of the complete graph is highlighted by black edges. </li>
<li>77 and 79 have high degrees, and can be seen as representative units for two clusters. </li>
<li>It seems to be a good idea to have another cluster, represented by unit 99.</li>
<li>Weakest links between such three units (two blue edges) are found. If they are removed, the tree becomes a forest with three components, which correspond to three clusters.</li>
</ul>
<p>{% include image.html max-width="600" file="/elec_consumption/./img/MST.png" %}</p>
<h3 id="How-to-Examine-Clustering-Results">How to Examine Clustering Results<a class="anchor-link" href="#How-to-Examine-Clustering-Results"> </a></h3><p>Most methods to validate cluster results are based on original data, so they cannot be used, either. Probably the only option left is to validate based on pair-wise similarity measures, like Euclidean distance and Pearson correlation.</p>
<p>The distance matrix can be sorted according to clusters. Correlations between members within clusters are supposed to have higher values. In constrast, off-diagonal block matrices represent inter-cluster correlations, which should be low and even negative.</p>
<h3 id="Results">Results<a class="anchor-link" href="#Results"> </a></h3><p>Clustering results based on <a href="https://github.com/edxu96/elec_consumption/tree/master/results/hourly.json">hourly profiles</a> and on <a href="https://github.com/edxu96/elec_consumption/tree/master/results/daily.json">daily profiles</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Regression">2. Regression<a class="anchor-link" href="#2.-Regression"> </a></h2><h3 id="Why-Facebook-Prophet">Why Facebook <code>Prophet</code><a class="anchor-link" href="#Why-Facebook-Prophet"> </a></h3><p><code>Prophet</code> is an open-source package for modelling and forecasting univariate time series by Facebook. Its model structure is similar to that of generalised additive models (GAM), where distinct non-linear terms are integrated in a linear way. <code>Prophet</code> works best when the series shows strong and multiple seasonality, and it is robust to missing values and effect from holidays, making it the perfect choice in our case.</p>
<h3 id="Seasonal-ARIMA">Seasonal ARIMA<a class="anchor-link" href="#Seasonal-ARIMA"> </a></h3><p>It is impossible to plot ACF and PACF for original series because of missing entries. As discussed before, they are adjacent so it is hard to fill with meaningful values. Result from <code>Prophet</code> provides a starting point. To have a weekly season is a good idea, and it should be "integrated", in order to remove the long-term trend. An <strong>integrated SARIMA (with AR1, MA1 and weekly seasonal AR1) structure</strong> is used for all daily profiles.</p>
<p>There are daily and weekly seasonal components in hourly profiles, which cannot be modelled by any seasonal ARIMA at the same time. So it cannot be used.</p>
<h3 id="Model-Validation">Model Validation<a class="anchor-link" href="#Model-Validation"> </a></h3><p>There are systematic ways for validation. It is challenging to conduct a cross validation for time series models, especially when there is long-term trend. <strong>Pseudo out-of-sample validation</strong> is used instead. Validation results show that seasonal ARIMA is good enough for daily profiles, compared to <code>Prophet</code>.</p>
<h3 id="Results">Results<a class="anchor-link" href="#Results"> </a></h3><p>Two sets of forecasts for 500 households are made <strong>Note that the physical unit is <a href="https://edxu96.github.io/elec_consumption/#1.-Introduction">presumed to be kiloWatt</a>.</strong></p>
<ul>
<li>3-step forecasts based on daily profile, done by integrated SARIMA (with AR1, MA1 and weekly seasonal AR1), in <a href="https://github.com/edxu96/elec_consumption/blob/master/results/three_step_daily.csv">a comma-separated CSV file</a></li>
<li>4-step forecasts based on hourly profile, done by Facebook <code>Prophet</code>, in <a href="https://github.com/edxu96/elec_consumption/blob/master/results/four_step_hourly.csv">another comma-separated CSV file</a></li>
</ul>

</div>
</div>
</div>
</div>
 

